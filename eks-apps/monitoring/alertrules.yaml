apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cluster-combined-alerts
  namespace: monitoring
  labels:
    release: monitoring    # MUST match Prometheus .spec.ruleSelector!
spec:
  groups:
    - name: node.rules
      rules:
        # Node Down
        - alert: NodeDown
          expr: up{job="node-exporter"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Node is down (instance {{ $labels.instance }})"
            description: "Node has been unreachable for 5m."
            runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/node_down"
        # Filesystem almost full
        - alert: NodeFilesystemAlmostFull
          expr: 100 - (node_filesystem_avail_bytes{mountpoint="/"} * 100 / node_filesystem_size_bytes{mountpoint="/"}) < 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Node filesystem almost full (instance {{ $labels.instance }})"
            description: "Less than 10% disk space left on root filesystem."
            runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/node/node_disk_full"
        # Node not ready
        - alert: KubeNodeNotReady
          expr: kube_node_status_condition{condition="Ready", status="true"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes node not ready"
            description: "Node {{ $labels.node }} is not Ready for 2m."
        # Node disk pressure
        - alert: KubeNodeDiskPressure
          expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Node has disk pressure"
            description: "Node {{ $labels.node }} is under disk pressure."
    - name: pod.rules
      rules:
        # Pod CrashLooping
        - alert: PodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod is crashlooping frequently (instance {{ $labels.instance }})"
            description: "Pod restart rate is high over the last 10 minutes."
        # Pod Not Ready
        - alert: PodNotReady
          expr: kube_pod_status_ready{condition="true"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod not ready (pod {{ $labels.pod }}, ns {{ $labels.namespace }})"
            description: "Pod has been in NotReady state for 5m."
        # Pod Not Healthy
        - alert: KubePodNotHealthy
          expr: sum by (namespace,pod) (kube_pod_status_phase{phase=~"Pending|Failed|Unknown"}) > 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Pod not healthy (instance {{ $labels.instance }})"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not running for >2m. VALUE={{ $value }} LABELS={{ $labels }}"
        # Pod CrashLooping (high rate)
        - alert: KubernetesPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[2m]) > 3
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Pod crash looping (instance {{ $labels.instance }})"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping. VALUE={{ $value }} LABELS={{ $labels }}"
        # DaemonSet rollout stuck
        - alert: KubernetesDaemonsetRolloutStuck
          expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "DaemonSet rollout stuck"
            description: "Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled or not ready. VALUE={{ $value }}"
        # Container OOMKiller
        - alert: KubernetesContainerOomKiller
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1)
            and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Container OOM Killed"
            description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was OOMKilled {{ $value }} times in 10m."
        # Pod OOMKilled
        - alert: PodOOMKilled
          expr: increase(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Pod OOMKilled (pod {{ $labels.pod }}, ns {{ $labels.namespace }})"
            description: "Pod was killed due to out of memory."
    - name: resource.rules
      rules:
        # High Pod CPU
        - alert: HighPodCPU
          expr: sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod,namespace) > 0.8
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod high CPU usage (pod {{ $labels.pod }}, ns {{ $labels.namespace }})"
            description: "Pod CPU usage is over 80% for 10m."
        # High Node Memory
        - alert: HighNodeMemory
          expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.15
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Node memory is low (instance {{ $labels.instance }})"
            description: "Node has less than 15% memory available."
        # High Container CPU
        - alert: ContainerHighCpuUtilization
          expr: (sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container!=""}/container_spec_cpu_period{container!=""}) by (pod, container) * 100) > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Container High CPU utilization"
            description: "Container CPU utilization > 80% for 2m. LABELS={{ $labels }}"
        # High Container Memory
        - alert: ContainerHighMemoryUsage
          expr: (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Container High Memory usage"
            description: "Container Memory usage > 80%. LABELS={{ $labels }}"
        # PV almost full
        - alert: KubePersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "PersistentVolume almost full"
            description: "Volume on node {{ $labels.instance }} is almost full (<10% left)."
    - name: traffic.rules
      rules:
        # High HTTP 5xx error rate
        - alert: HighErrorRate
          expr: sum(rate(http_requests_total{status=~"5.."}[5m])) by (job) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High HTTP 5xx error rate (job {{ $labels.job }})"
            description: "More than 0.05 5xx errors/sec for 5 minutes."
        # High HTTP request latency
        - alert: HighRequestLatency
          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{handler="/api"}[5m])) by (le,job)) > 0.5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High request latency on /api (job {{ $labels.job }})"
            description: "95th percentile request latency > 0.5s over last 10m."
    - name: controlplane.rules
      rules:
        # API server down
        - alert: KubeAPIDown
          expr: absent(up{job="apiserver"})
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes API server is down"
            description: "No API server targets are up."
        # Scheduler down
        - alert: KubeSchedulerDown
          expr: absent(up{job="kube-scheduler"})
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes scheduler is down"
            description: "No scheduler targets are up."
        # Controller manager down
        - alert: KubeControllerManagerDown
          expr: absent(up{job="kube-controller-manager"})
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes controller manager is down"
            description: "No controller manager targets are up."
        # API server errors
        - alert: KubeApiServerErrors
          expr: sum(rate(apiserver_request_total{code=~"5.."}[5m])) by (instance) > 1
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes API server errors"
            description: "API server instance {{ $labels.instance }} has error rate > 1/s over 5m."
