name: Infra + EKS Deploy (One Flow)

on:
  push:
    branches: [ main ]
    paths:
      - 'Infra/**'
      - 'eks-apps/**'
      - '.github/workflows/infra-and-deploy.yml'
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  # Prefer repo SECRETS, then VARS, then defaults
  AWS_REGION: ${{ secrets.AWS_REGION != '' && secrets.AWS_REGION || vars.AWS_REGION != '' && vars.AWS_REGION || 'ap-south-1' }}
  EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME || vars.EKS_CLUSTER_NAME || 'ushasreestores-eks' }}
  TF_IN_AUTOMATION: true
  TF_INPUT: 0
  TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET || vars.TF_STATE_BUCKET || 'ushasreestores-s3-tfstate' }}
  TF_STATE_DYNAMODB_TABLE: ${{ secrets.TF_STATE_DYNAMODB_TABLE || vars.TF_STATE_DYNAMODB_TABLE || 'ushasreestores-s3-tflock' }}
  TF_STATE_KEY: ${{ secrets.TF_STATE_KEY || 'infra/terraform.tfstate' }}

jobs:
  infra_and_deploy:
    runs-on: ubuntu-latest
    concurrency:
      group: infra-and-deploy-${{ github.ref_name }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # If you use OIDC role: uncomment this and remove access keys from secrets
      # - name: Configure AWS credentials (OIDC)
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
      #     aws-region: ${{ env.AWS_REGION }}

      # If you're using ACCESS KEYS in repo secrets (as your logs show):
      - name: Configure AWS credentials (Access Keys)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      # ---- Optional fmt check (won’t fail pipeline on code 3) ----
      - name: Terraform fmt (non-blocking)
        working-directory: Infra
        run: |
          set +e
          terraform fmt -check -recursive
          status=$?
          if [ $status -eq 3 ]; then
            echo "::warning::Terraform files need formatting. Run 'terraform fmt -recursive' locally."
            terraform fmt -diff -recursive || true
            exit 0
          fi
          exit $status

      # ---- Phase 1: bootstrap state bucket + lock table using LOCAL backend ----
      - name: Terraform init (local backend)
        working-directory: Infra
        run: terraform init -reconfigure -backend=false

      - name: Create S3 & DynamoDB for backend (bootstrap)
        working-directory: Infra
        run: |
          set -euo pipefail
          terraform apply -auto-approve \
            -target=aws_s3_bucket.tfstate \
            -target=aws_s3_bucket_versioning.tfstate \
            -target=aws_s3_bucket_server_side_encryption_configuration.tfstate \
            -target=aws_dynamodb_table.tf_lock

      # ---- Phase 2: re-init to S3 backend (this was missing!) ----
      - name: Terraform init (S3 backend, reconfigure)
        working-directory: Infra
        run: |
          set -euo pipefail
          terraform init -reconfigure \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="key=${TF_STATE_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_STATE_DYNAMODB_TABLE}" \
            -backend-config="encrypt=true"

      - name: Terraform validate
        working-directory: Infra
        run: terraform validate

      # ---- Phase 3: full apply (VPC/EKS/RDS/…) ----
      - name: Terraform apply (full infra)
        working-directory: Infra
        run: terraform apply -auto-approve

      # ---- EKS deploy steps ----
      - name: Wait for EKS ACTIVE
        run: aws eks wait cluster-active --name "${{ env.EKS_CLUSTER_NAME }}"

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: 'latest'

      - name: Update kubeconfig & quick check
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${{ env.EKS_CLUSTER_NAME }}" --region "${{ env.AWS_REGION }}"
          kubectl version --short || true
          kubectl get nodes -o wide

      - name: Make scripts executable
        run: |
          chmod +x eks-apps/deployapps.sh || true
          chmod +x eks-apps/appsdeploy.sh || true
          chmod +x eks-apps/argocd/argocd.sh || true
          chmod +x eks-apps/nginx-ingress/*.sh || true
          chmod +x eks-apps/monitoring/monitoring.sh || true

      - name: Deploy apps (deployapps.sh)
        run: |
          set -euo pipefail
          ./eks-apps/deployapps.sh

      - name: Post-deploy sanity
        run: |
          kubectl get ns
          kubectl get pods -A
          kubectl get svc -A
          kubectl get ingress -A

      - name: Collect diagnostics on failure
        if: failure()
        run: |
          mkdir -p diag
          kubectl get nodes -o wide > diag/nodes.txt || true
          kubectl get pods -A -o wide > diag/pods.txt || true
          kubectl describe pods -A > diag/pods_describe.txt || true
          kubectl get events -A --sort-by=.lastTimestamp > diag/events.txt || true
          kubectl get ing -A -o yaml > diag/ingresses.yaml || true
          kubectl get svc -A -o yaml > diag/services.yaml || true

      - name: Upload diagnostics
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: k8s-diagnostics
          path: diag/
